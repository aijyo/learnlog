#include <algorithm>
#include <cstdint>
#include <iostream>
#include <limits>
#include <numeric>
#include <stdexcept>
#include <vector>

// ------------------------------ Tensor (row-major) ------------------------------
//
// A minimal N-D tensor container:
// - shape: e.g., [N, C, H, W]
// - data: flat storage in row-major order
//
// This is intentionally lightweight for demonstrating Slice.
//
template <typename T>
struct Tensor {
  std::vector<int64_t> shape;
  std::vector<T> data;

  int64_t rank() const { return static_cast<int64_t>(shape.size()); }

  int64_t numel() const {
    if (shape.empty()) return 0;
    int64_t n = 1;
    for (int64_t d : shape) {
      if (d < 0) throw std::runtime_error("Negative dimension is not supported.");
      n *= d;
    }
    return n;
  }
};

// ------------------------------ Utilities ------------------------------

// Compute row-major strides for a given shape.
// strides[i] = product(shape[i+1:])
static inline std::vector<int64_t> compute_strides_row_major(const std::vector<int64_t>& shape) {
  const int64_t r = static_cast<int64_t>(shape.size());
  std::vector<int64_t> strides(r, 1);
  for (int64_t i = r - 2; i >= 0; --i) {
    strides[i] = strides[i + 1] * shape[i + 1];
  }
  return strides;
}

static inline int64_t clamp_i64(int64_t v, int64_t lo, int64_t hi) {
  return std::max(lo, std::min(v, hi));
}

// Normalize an axis: allow negative axis indexing.
static inline int64_t normalize_axis(int64_t axis, int64_t rank) {
  if (axis < 0) axis += rank;
  if (axis < 0 || axis >= rank) throw std::runtime_error("Axis out of range.");
  return axis;
}

// Compute the length of indices generated by (start, end, step) with ONNX semantics:
// - If step > 0: idx = start, start+step, ... while idx < end
// - If step < 0: idx = start, start+step, ... while idx > end
static inline int64_t slice_dim_length(int64_t start, int64_t end, int64_t step) {
  if (step == 0) throw std::runtime_error("Slice step cannot be zero.");
  if (step > 0) {
    if (start >= end) return 0;
    // count = ceil((end - start) / step)
    const int64_t diff = end - start;               // positive
    return (diff + step - 1) / step;
  } else {
    // step < 0
    if (start <= end) return 0;
    const int64_t stepAbs = -step;
    const int64_t diff = start - end;               // positive
    return (diff + stepAbs - 1) / stepAbs;
  }
}

// ------------------------------ ONNX Slice Implementation ------------------------------
//
// Implements ONNX Slice (runtime) following the same normalization rules as in onnx-mlir snippet:
// - axes optional: if omitted, default axes = [0, 1, ..., k-1] where k = len(starts)
// - steps optional: if omitted, default all 1
// - start/end are normalized with negative indexing and clamped depending on step sign
// - end supports int32 min/max as -inf/+inf sentinels
//
// Returns a newly allocated output tensor.
//
template <typename T>
Tensor<T> onnx_slice(const Tensor<T>& data,
                     const std::vector<int64_t>& starts,
                     const std::vector<int64_t>& ends,
                     const std::vector<int64_t>* axes_opt = nullptr,
                     const std::vector<int64_t>* steps_opt = nullptr) {
  const int64_t rank = data.rank();
  if (rank <= 0) throw std::runtime_error("Slice requires a ranked tensor.");

  if (static_cast<int64_t>(starts.size()) != static_cast<int64_t>(ends.size()))
    throw std::runtime_error("starts and ends must have the same length.");

  const int64_t k = static_cast<int64_t>(starts.size());
  if (k == 0) {
    // No slicing info -> return a copy (common behavior)
    return data;
  }

  // Build axes: if omitted, default to [0..k-1]
  std::vector<int64_t> axes(k);
  if (!axes_opt) {
    if (k > rank) throw std::runtime_error("Default axes would exceed data rank.");
    for (int64_t i = 0; i < k; ++i) axes[i] = i;
  } else {
    if (static_cast<int64_t>(axes_opt->size()) != k)
      throw std::runtime_error("axes length must match starts/ends length.");
    for (int64_t i = 0; i < k; ++i) axes[i] = normalize_axis((*axes_opt)[i], rank);
  }

  // Build steps: if omitted, default to all 1
  std::vector<int64_t> steps(k, 1);
  if (steps_opt) {
    if (static_cast<int64_t>(steps_opt->size()) != k)
      throw std::runtime_error("steps length must match starts/ends length.");
    for (int64_t i = 0; i < k; ++i) {
      if ((*steps_opt)[i] == 0) throw std::runtime_error("steps cannot contain zero.");
      steps[i] = (*steps_opt)[i];
    }
  }

  // Default per-axis slice params (for non-mentioned axes)
  std::vector<int64_t> startFinal(rank, 0);
  std::vector<int64_t> endFinal(rank, 0);
  std::vector<int64_t> stepFinal(rank, 1);

  // Initialize endFinal default as dim (full range)
  for (int64_t d = 0; d < rank; ++d) endFinal[d] = data.shape[d];

  // Apply ONNX normalization for each mentioned axis
  const int64_t negInf = std::numeric_limits<int32_t>::min();
  const int64_t posInf = std::numeric_limits<int32_t>::max();

  for (int64_t i = 0; i < k; ++i) {
    const int64_t axis = axes[i];
    const int64_t dim = data.shape[axis];
    if (dim < 0) throw std::runtime_error("Dynamic/negative dim is not supported in this runtime demo.");

    const int64_t sIn = starts[i];
    const int64_t eIn = ends[i];
    const int64_t st = steps[i];

    // startPos = (start < 0) ? start + dim : start
    int64_t startPos = (sIn < 0) ? (sIn + dim) : sIn;

    // If step < 0: clamp(0, start, dim-1), else clamp(0, start, dim)
    int64_t sFin = 0;
    if (st < 0) sFin = clamp_i64(startPos, 0, dim - 1);
    else        sFin = clamp_i64(startPos, 0, dim);

    // endPos = (end < 0) ? end + dim : end
    int64_t endPos = (eIn < 0) ? (eIn + dim) : eIn;

    // Handle -inf/+inf sentinels
    if (eIn <= negInf) endPos = -1;
    if (eIn >= posInf) endPos = dim;

    // If step < 0: clamp(-1, end, dim), else clamp(0, end, dim)
    int64_t eFin = 0;
    if (st < 0) eFin = clamp_i64(endPos, -1, dim);
    else        eFin = clamp_i64(endPos, 0, dim);

    startFinal[axis] = sFin;
    endFinal[axis] = eFin;
    stepFinal[axis] = st;
  }

  // Compute output shape
  Tensor<T> out;
  out.shape = data.shape;

  for (int64_t axis = 0; axis < rank; ++axis) {
    const int64_t len = slice_dim_length(startFinal[axis], endFinal[axis], stepFinal[axis]);
    out.shape[axis] = len;
  }

  // Allocate output
  out.data.resize(out.numel());

  // Early exit for empty output
  if (out.data.empty()) return out;

  // Prepare strides
  const auto inStrides = compute_strides_row_major(data.shape);
  const auto outStrides = compute_strides_row_major(out.shape);

  // Iterate over all output elements using a flat index -> multi-index conversion
  // and map each output multi-index to input multi-index via:
  // in_idx[axis] = startFinal[axis] + out_idx[axis] * stepFinal[axis]
  const int64_t outNumel = out.numel();
  const int64_t r = rank;

  for (int64_t outLinear = 0; outLinear < outNumel; ++outLinear) {
    int64_t tmp = outLinear;
    int64_t inOffset = 0;

    for (int64_t axis = 0; axis < r; ++axis) {
      const int64_t coord = (outStrides[axis] == 0) ? 0 : (tmp / outStrides[axis]);
      tmp = (outStrides[axis] == 0) ? 0 : (tmp % outStrides[axis]);

      const int64_t inCoord = startFinal[axis] + coord * stepFinal[axis];
      inOffset += inCoord * inStrides[axis];
    }

    out.data[outLinear] = data.data[inOffset];
  }

  return out;
}

// ------------------------------ Demo ------------------------------
static void print_tensor_3d_i64(const Tensor<int64_t>& t, const std::string& name) {
  std::cout << name << " shape = [";
  for (size_t i = 0; i < t.shape.size(); ++i) {
    std::cout << t.shape[i] << (i + 1 == t.shape.size() ? "" : ", ");
  }
  std::cout << "]\n";

  if (t.shape.size() != 3) {
    std::cout << "(print demo only supports 3D)\n";
    return;
  }

  const int64_t D0 = t.shape[0], D1 = t.shape[1], D2 = t.shape[2];
  const auto strides = compute_strides_row_major(t.shape);

  for (int64_t i = 0; i < D0; ++i) {
    std::cout << "  [" << i << "]\n";
    for (int64_t j = 0; j < D1; ++j) {
      std::cout << "    ";
      for (int64_t k = 0; k < D2; ++k) {
        const int64_t off = i * strides[0] + j * strides[1] + k * strides[2];
        std::cout << t.data[off] << (k + 1 == D2 ? "" : ", ");
      }
      std::cout << "\n";
    }
  }
}

int main() {
  // Build a 3D tensor: shape [2, 3, 4], values 0..23
  Tensor<int64_t> x;
  x.shape = {2, 3, 4};
  x.data.resize(x.numel());
  std::iota(x.data.begin(), x.data.end(), 0);

  print_tensor_3d_i64(x, "x");

  // Example 1:
  // Slice axes [1,2]:
  // - axis=1: start=1, end=3, step=1  -> take j = 1,2
  // - axis=2: start=0, end=4, step=2  -> take k = 0,2
  std::vector<int64_t> starts1 = {1, 0};
  std::vector<int64_t> ends1   = {3, 4};
  std::vector<int64_t> axes1   = {1, 2};
  std::vector<int64_t> steps1  = {1, 2};

  auto y1 = onnx_slice(x, starts1, ends1, &axes1, &steps1);
  print_tensor_3d_i64(y1, "y1 (axes=[1,2], starts=[1,0], ends=[3,4], steps=[1,2])");

  // Example 2 (negative axis and negative start/end):
  // axes = [-1] means axis=2 for rank=3
  // start=-3 -> start+dim(4)=1, end=-1 -> end+dim(4)=3, step=1 => take k=1,2
  std::vector<int64_t> starts2 = {-3};
  std::vector<int64_t> ends2   = {-1};
  std::vector<int64_t> axes2   = {-1};

  auto y2 = onnx_slice(x, starts2, ends2, &axes2, nullptr);
  print_tensor_3d_i64(y2, "y2 (axes=[-1], starts=[-3], ends=[-1], steps=default)");

  // Example 3 (negative step):
  // axis=2: start=3, end=-1 (=> 3), step=-1
  // With step<0, end is clamped to [-1, dim], and the iteration is idx > end
  // Here: dim=4, end=-1 => end+dim=3, so range becomes from 3 down to >3 => empty
  // Let's use end=-5 (<= -inf is special; but -5 => -1 after +dim? actually -5+4=-1)
  // start=3, end=-5 => endPos = -1, step=-1 => idx = 3,2,1,0 (since idx > -1)
  std::vector<int64_t> starts3 = {3};
  std::vector<int64_t> ends3   = {-5};
  std::vector<int64_t> axes3   = {2};
  std::vector<int64_t> steps3  = {-1};

  auto y3 = onnx_slice(x, starts3, ends3, &axes3, &steps3);
  print_tensor_3d_i64(y3, "y3 (axis=2, start=3, end=-5, step=-1)");

  return 0;
}
