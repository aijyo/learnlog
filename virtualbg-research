将模型转为onnx模式，然后使用onnx runtime: https://onnxruntime.ai/
#include <onnxruntime_cxx_api.h>
#include "model.h"
#include <iostream>
#include <opencv2/videoio.hpp>
#include <opencv2/highgui.hpp>
#include <locale>
#include <codecvt>
#include <string>

struct background_removal_filter
{
    std::unique_ptr<Ort::Session> session;
    std::unique_ptr<Ort::Env> env;
    std::vector<Ort::AllocatedStringPtr> inputNames;
    std::vector<Ort::AllocatedStringPtr> outputNames;
    std::vector<Ort::Value> inputTensor;
    std::vector<Ort::Value> outputTensor;
    std::vector<std::vector<int64_t>> inputDims;
    std::vector<std::vector<int64_t>> outputDims;
    std::vector<std::vector<float>> outputTensorValues;
    std::vector<std::vector<float>> inputTensorValues;
    float threshold = 0.5f;
    cv::Scalar backgroundColor{0, 0, 0};
    float contourFilter = 0.05f;
    float smoothContour = 0.5f;
    float feather = 0.0f;
    std::string useGPU;
    std::string modelSelection;
    std::unique_ptr<Model> model;

    // // Use the media-io converter to both scale and convert the colorspace
    // video_scaler_t *scalerToBGR;
    // video_scaler_t *scalerFromBGR;

    cv::Mat backgroundMask;
    int maskEveryXFrames = 1;
    int maskEveryXFramesCount = 0;
    std::vector<std::string> xxx;

#if _WIN32
    const wchar_t *modelFilepath = nullptr;
#else
    const char *modelFilepath = nullptr;
#endif
};

const char *MODEL_SINET_NAME = "SINET";
const char *MODEL_MODNET_NAME = "MODNET";
const char *MODEL_MEDIAPIPE_NAME = "MEDIAPIPE";
const char *MODEL_SELFIE_NAME = "SELFIE";
const char *MODEL_RVM_NAME = "RVM";

const char *MODEL_SINET = "models/SINet_Softmax_simple.onnx";
const char *MODEL_MODNET = "models/modnet_simple.onnx";
const char *MODEL_MEDIAPIPE = "models/mediapipe.onnx";
const char *MODEL_SELFIE = "models/selfie_segmentation.onnx";
const char *MODEL_RVM = "models/rvm_mobilenetv3_fp32.onnx";

const char *USEGPU_CPU = "cpu";
const char *USEGPU_DML = "dml";
static void processImageForBackground(struct background_removal_filter *tf, const cv::Mat &imageBGR,
                                      cv::Mat &backgroundMask);
static cv::Mat render(struct background_removal_filter *tf, cv::Mat frame);

int main()
{
    std::cout << "> SINET has the opposite foreground and background; MODNET has the worst effect; RVM has the best effect and the highest resource consumption; the other two are moderate." << std::endl;

    std::cout << "> The first time to load the model will be more time-consuming (20s)" << std::endl;

    std::cout << std::endl;
    std::cout << std::endl;
    std::cout << "> Please select the model (SINET/MODNET/MEDIAPIPE/SELFIE/RVM)" << std::endl;
    std::string line;
    std::string model = MODEL_SELFIE;
    while (std::getline(std::cin, line))
    {
        if (line == MODEL_SINET_NAME)
        {
            model = MODEL_SINET;
        }
        else if (line == MODEL_MODNET_NAME)
        {
            model = MODEL_MODNET;
        }
        else if (line == MODEL_MEDIAPIPE_NAME)
        {
            model = MODEL_MEDIAPIPE;
        }
        else if (line == MODEL_SELFIE_NAME)
        {
            model = MODEL_SELFIE;
        }
        else if (line == MODEL_RVM_NAME)
        {
            model = MODEL_RVM;
        }
        else
        {
            std::cout << "Invalid input" << std::endl;
            continue;
        }
        std::cout << "You choose " << model << std::endl;
        break;
    }

    boolean enableGpu = true;
    std::cout << "> Whether to use gpu or not. (0 / 1)" << std::endl;
    while (std::getline(std::cin, line))
    {
        if (line == "0")
        {
            enableGpu = false;
        }
        else if (line == "1")
        {
            enableGpu = true;
        }
        else
        {
            std::cout << "Invalid input" << std::endl;
            continue;
        }
        std::cout << "You use" << (enableGpu ? "GPU" : "CPU") << std::endl;
        break;
    }

    struct background_removal_filter *tf = new background_removal_filter;

    std::string instanceName{"background-removal-inference"};
    tf->env.reset(new Ort::Env(OrtLoggingLevel::ORT_LOGGING_LEVEL_ERROR, instanceName.c_str()));

    tf->maskEveryXFrames = 1;
    tf->backgroundColor = {0, 0, 0};
    tf->threshold = 0.5f;
    tf->contourFilter = 0.05f;
    tf->smoothContour = 0.5f;
    tf->feather = 0.0f;
    tf->modelSelection = model;
    tf->useGPU = enableGpu ? std::string(USEGPU_DML) : std::string(USEGPU_CPU);

    if (tf->modelSelection == MODEL_SINET)
    {
        tf->model.reset(new ModelSINET);
    }
    if (tf->modelSelection == MODEL_MODNET)
    {
        tf->model.reset(new ModelMODNET);
    }
    if (tf->modelSelection == MODEL_SELFIE)
    {
        tf->model.reset(new ModelSelfie);
    }
    if (tf->modelSelection == MODEL_MEDIAPIPE)
    {
        tf->model.reset(new ModelMediaPipe);
    }
    if (tf->modelSelection == MODEL_RVM)
    {
        tf->model.reset(new ModelRVM);
    }

    std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;
    std::wstring wide = converter.from_bytes(tf->modelSelection);

    tf->modelFilepath = wide.c_str();

    Ort::SessionOptions sessionOptions;
    sessionOptions.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);
    if (tf->useGPU != USEGPU_CPU)
    {
        sessionOptions.DisableMemPattern();
        sessionOptions.SetExecutionMode(ExecutionMode::ORT_SEQUENTIAL);
    }

    if (tf->useGPU == USEGPU_DML)
    {
        Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_DML(sessionOptions, 0));
    }
    tf->session.reset(new Ort::Session(*tf->env, tf->modelFilepath, sessionOptions));

    Ort::AllocatorWithDefaultOptions allocator;

    tf->model->populateInputOutputNames(tf->session, tf->inputNames, tf->outputNames);

    if (!tf->model->populateInputOutputShapes(tf->session, tf->inputDims, tf->outputDims))
    {
        return 1;
    }

    // Allocate buffers
    tf->model->allocateTensorBuffers(tf->inputDims, tf->outputDims, tf->outputTensorValues,
                                     tf->inputTensorValues, tf->inputTensor, tf->outputTensor);

    cv::Mat frame;
    //--- INITIALIZE VIDEOCAPTURE
    cv::VideoCapture cap;
    // open the default camera using default API
    // cap.open(0);
    // OR advance usage: select any API backend
    int deviceID = 0;        // 0 = open default camera
    int apiID = cv::CAP_ANY; // 0 = autodetect default API
    // open selected camera using selected API
    cap.open(deviceID, apiID);
    // check if we succeeded
    if (!cap.isOpened())
    {
        std::cerr << "ERROR! Unable to open camera\n";
        return -1;
    }

    //--- GRAB AND WRITE LOOP
    std::cout << "Start grabbing" << std::endl
              << "Press any key to terminate" << std::endl;
    for (;;)
    {
        // wait for a new frame from camera and store it into 'frame'
        cap.read(frame);
        // check if we succeeded
        if (frame.empty())
        {
            std::cerr << "ERROR! blank frame grabbed\n";
            break;
        }
        // show live and wait for a key with timeout long enough to show images
        cv::imshow("Live", render(tf, frame));
        if (cv::waitKey(5) >= 0)
            break;
    }
    return 0;
}

static cv::Mat render(struct background_removal_filter *tf, cv::Mat frame)
{
    cv::Mat backgroundMask(frame.size(), CV_8UC1, cv::Scalar(255));

    tf->maskEveryXFramesCount++;
    tf->maskEveryXFramesCount %= tf->maskEveryXFrames;
    if (tf->maskEveryXFramesCount != 0 && !tf->backgroundMask.empty())
    {
        // We are skipping processing of the mask for this frame.
        // Get the background mask previously generated.
        return frame;
    }
    else
    {
        processImageForBackground(tf, frame, backgroundMask);
        frame.setTo(tf->backgroundColor, backgroundMask);
        return frame;
    }
}

static void processImageForBackground(struct background_removal_filter *tf, const cv::Mat &imageBGR,
                                      cv::Mat &backgroundMask)
{
    if (tf->session.get() == nullptr)
    {
        // Onnx runtime session is not initialized. Problem in initialization
        return;
    }
    try
    {
        // To RGB
        cv::Mat imageRGB;
        cv::cvtColor(imageBGR, imageRGB, cv::COLOR_BGR2RGB);

        // Resize to network input size
        uint32_t inputWidth, inputHeight;
        tf->model->getNetworkInputSize(tf->inputDims, inputWidth, inputHeight);

        cv::Mat resizedImageRGB;
        cv::resize(imageRGB, resizedImageRGB, cv::Size(inputWidth, inputHeight));

        // Prepare input to nework
        cv::Mat resizedImage, preprocessedImage;
        resizedImageRGB.convertTo(resizedImage, CV_32F);

        tf->model->prepareInputToNetwork(resizedImage, preprocessedImage);

        tf->model->loadInputToTensor(preprocessedImage, inputWidth, inputHeight, tf->inputTensorValues);

        // Run network inference
        tf->model->runNetworkInference(tf->session, tf->inputNames, tf->outputNames, tf->inputTensor,
                                       tf->outputTensor);

        // Get output
        // Map network output mask to cv::Mat
        cv::Mat outputImage = tf->model->getNetworkOutput(tf->outputDims, tf->outputTensorValues);

        // Assign output to input in some models that have temporal information
        tf->model->assignOutputToInput(tf->outputTensorValues, tf->inputTensorValues);

        // Post-process output
        tf->model->postprocessOutput(outputImage);

        if (tf->modelSelection == MODEL_MEDIAPIPE)
        {
            backgroundMask = outputImage > tf->threshold;
        }
        else
        {
            backgroundMask = outputImage < tf->threshold;
        }

        // Contour processing
        if (tf->contourFilter > 0.0 && tf->contourFilter < 1.0)
        {
            std::vector<std::vector<cv::Point>> contours;
            findContours(backgroundMask, contours, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE);
            std::vector<std::vector<cv::Point>> filteredContours;
            const int64_t contourSizeThreshold = (int64_t)(backgroundMask.total() * tf->contourFilter);
            for (auto &contour : contours)
            {
                if (cv::contourArea(contour) > contourSizeThreshold)
                {
                    filteredContours.push_back(contour);
                }
            }
            backgroundMask.setTo(0);
            drawContours(backgroundMask, filteredContours, -1, cv::Scalar(255), -1);
        }

        // Resize the size of the mask back to the size of the original input.
        cv::resize(backgroundMask, backgroundMask, imageBGR.size());

        // Smooth mask with a fast filter (box).
        if (tf->smoothContour > 0.0)
        {
            int k_size = (int)(100 * tf->smoothContour);
            cv::boxFilter(backgroundMask, backgroundMask, backgroundMask.depth(),
                          cv::Size(k_size, k_size));
            backgroundMask = backgroundMask > 128;
        }
    }
    catch (const std::exception &e)
    {
    }
}
